#!/usr/bin/env python3
from __future__ import annotations

import ast
import json
import os
import re
import subprocess
import sys
import shutil
from pathlib import Path

FISH_CONFIG = """\
# default fish config for the devcontainer
set -g __fish_git_prompt_showdirtystate 0
set -g __fish_git_prompt_showuntrackedfiles 0
set -g __fish_git_prompt_showupstream none

function fish_greeting
  echo "agents devcontainer Â· autonomous coding sandbox"
end

function fish_prompt
  set_color cyan
  echo -n (prompt_pwd)
  set_color normal
  fish_vcs_prompt
  echo -n " > "
end

alias codexxx "codex --dangerously-bypass-approvals-and-sandbox"
alias clod "claude --dangerously-skip-permissions"
"""

TMUX_CONFIG = """\
set -g default-terminal "tmux-256color"
set -g focus-events on
set -sg escape-time 10
set -g mouse on
set -g history-limit 200000
set -g renumber-windows on
setw -g mode-keys vi

# Keep new panes/windows in the same cwd
bind c new-window -c "#{pane_current_path}"
bind | split-window -h -c "#{pane_current_path}"
bind - split-window -v -c "#{pane_current_path}"
unbind '"'
unbind %

# Reload config
bind r source-file ~/.tmux.conf \\; display-message "tmux.conf reloaded"

# Terminal features
set -as terminal-features ",xterm-ghostty:RGB"
set -as terminal-features ",xterm*:RGB"
set -ga terminal-overrides ",xterm*:colors=256"
set -ga terminal-overrides '*:Ss=\\E[%p1%d q:Se=\\E[ q'
"""

SHELL_ALIASES = """\
# agent aliases (devcontainer)
alias codexxx='codex --dangerously-bypass-approvals-and-sandbox'
alias clod='claude --dangerously-skip-permissions'
"""

CODEX_CONFIG_HEADER = "# generated by devcontainer post_install.py (container override)"
CODEX_MCP_HOST = "host.docker.internal"
CODEX_MCP_URL_RE = re.compile(
    r'^(?P<prefix>\s*url\s*=\s*")(?P<scheme>https?|wss?)://'
    r'(?P<host>localhost|127\\.0\\.0\\.1|\\[::1\\]|::1)(?P<rest>[^"]*)(?P<suffix>".*)$'
)
CODEX_MCP_SECTION_RE = re.compile(r"^\[(?P<section>[^\]]+)\]\s*$")
CODEX_WEB_SEARCH_REQUEST_RE = re.compile(r"^(?P<indent>\s*)web_search_request\s*=\s*(?P<value>.+?)\s*$")
CODEX_MCP_STARTUP_TIMEOUT_SEC = 60
DEVC_DISABLE_MCP_SERVERS_ENV = "DEVC_DISABLE_MCP_SERVERS"


def log(message: str) -> None:
    print(f"post-install: {message}", file=sys.stderr)


def run_git(
    args: list[str], cwd: Path, check: bool = False
) -> subprocess.CompletedProcess[str]:
    return subprocess.run(
        ["git", "-C", str(cwd), *args],
        check=check,
        capture_output=True,
        text=True,
    )


def run_sudo(args: list[str]) -> subprocess.CompletedProcess[str]:
    return subprocess.run(
        ["sudo", *args],
        check=False,
        capture_output=True,
        text=True,
    )


def resolve_workspace() -> Path:
    env_workspace = os.environ.get("WORKSPACE_FOLDER")
    if env_workspace:
        workspace = Path(env_workspace)
    else:
        workspace = Path("/workspace")
    if workspace.exists():
        return workspace
    return Path.cwd()


def is_git_repo(cwd: Path) -> bool:
    result = run_git(["rev-parse", "--is-inside-work-tree"], cwd)
    return result.returncode == 0 and result.stdout.strip() == "true"


def ensure_global_gitignore(workspace: Path) -> None:
    result = run_git(["config", "--global", "--path", "core.excludesfile"], workspace)
    if result.returncode != 0:
        log("no global core.excludesfile configured")
        return

    raw_path = result.stdout.strip()
    if not raw_path:
        log("no global core.excludesfile configured")
        return

    excludes_path = Path(raw_path).expanduser()
    if not excludes_path.is_absolute():
        excludes_path = (Path.home() / excludes_path).resolve()

    if excludes_path.exists():
        log(f"global core.excludesfile exists at {excludes_path}")
        return

    source = workspace / ".devcontainer" / ".gitignore_global"
    if not source.exists():
        log(
            f"global core.excludesfile missing at {excludes_path} and no template copy found"
        )
        return

    excludes_path.parent.mkdir(parents=True, exist_ok=True)
    shutil.copyfile(source, excludes_path)
    log(f"copied gitignore to {excludes_path}")


def upsert_root_key(text: str, key: str, value: str) -> tuple[str, bool]:
    lines = text.splitlines()
    pattern = re.compile(rf"^\s*{re.escape(key)}\s*=")
    for i, line in enumerate(lines):
        if pattern.match(line):
            new_line = f'{key} = "{value}"'
            if line.strip() == new_line:
                return text, False
            lines[i] = new_line
            return "\n".join(lines).rstrip() + "\n", True

    insert_at = len(lines)
    for i, line in enumerate(lines):
        if line.strip().startswith("["):
            insert_at = i
            break
    lines.insert(insert_at, f'{key} = "{value}"')
    return "\n".join(lines).rstrip() + "\n", True


def rewrite_mcp_urls(text: str) -> tuple[str, bool]:
    changed = False
    out_lines: list[str] = []
    for line in text.splitlines():
        match = CODEX_MCP_URL_RE.match(line)
        if match:
            new_line = (
                f'{match.group("prefix")}{match.group("scheme")}://'
                f"{CODEX_MCP_HOST}{match.group('rest')}{match.group('suffix')}"
            )
            if new_line != line:
                changed = True
                out_lines.append(new_line)
                continue
        out_lines.append(line)
    if not changed:
        return text, False
    return "\n".join(out_lines).rstrip() + "\n", True


def rewrite_web_search_feature(text: str) -> tuple[str, bool]:
    in_features = False
    updated = False
    out_lines: list[str] = []
    for line in text.splitlines():
        stripped = line.strip()
        if stripped.startswith("["):
            in_features = stripped == "[features]"
            out_lines.append(line)
            continue

        if in_features:
            match = CODEX_WEB_SEARCH_REQUEST_RE.match(line)
            if match:
                value = match.group("value").strip().strip('"').strip().lower()
                if value in {"true", "1", "yes", "on"}:
                    replacement = "live"
                elif value in {"false", "0", "no", "off"}:
                    replacement = "disabled"
                elif value in {"live", "cached", "disabled"}:
                    replacement = value
                else:
                    replacement = "live"

                out_lines.append(f'{match.group("indent")}web_search = "{replacement}"')
                updated = True
                continue

        out_lines.append(line)

    if not updated:
        return text, False
    return "\n".join(out_lines).rstrip() + "\n", True


def _parse_toml_array(value: str) -> list[str] | None:
    try:
        parsed = json.loads(value)
    except json.JSONDecodeError:
        return None
    if not isinstance(parsed, list):
        return None
    return [str(item) for item in parsed]


def _find_section_range(lines: list[str], section_name: str) -> tuple[int | None, int]:
    section_header = f"[{section_name}]"
    start = None
    for i, line in enumerate(lines):
        if line.strip() == section_header:
            start = i
            break
    if start is None:
        return None, -1
    end = len(lines)
    for j in range(start + 1, len(lines)):
        if CODEX_MCP_SECTION_RE.match(lines[j]) and lines[j].strip() != section_header:
            end = j
            break
    return start, end


def _get_disabled_mcp_servers() -> set[str]:
    raw = os.environ.get(DEVC_DISABLE_MCP_SERVERS_ENV, "")
    if not raw:
        return set()

    disabled: set[str] = set()
    for item in raw.replace(";", ",").replace(" ", ",").split(","):
        name = item.strip().lower()
        if not name:
            continue
        if name.startswith("mcp_servers."):
            name = name.removeprefix("mcp_servers.")
        disabled.add(name)
    return disabled


def _is_disabled_mcp_section(section_name: str, disabled: set[str]) -> bool:
    if not disabled:
        return False

    normalized = section_name.strip().lower()
    if normalized in disabled:
        return True
    if normalized.startswith("mcp_servers."):
        return normalized.removeprefix("mcp_servers.") in disabled
    return False


def _filter_disabled_mcp_sections(lines: list[str], disabled: set[str]) -> tuple[list[str], bool]:
    if not disabled:
        return lines, False

    out: list[str] = []
    removed = False
    i = 0
    while i < len(lines):
        line = lines[i]
        match = CODEX_MCP_SECTION_RE.match(line)
        if match and _is_disabled_mcp_section(match.group("section"), disabled):
            removed = True
            i += 1
            while i < len(lines) and not CODEX_MCP_SECTION_RE.match(lines[i]):
                i += 1
            continue
        out.append(line)
        i += 1

    return out, removed


def _ensure_section_key_int(lines: list[str], start: int, end: int, key: str, value: int) -> bool:
    for i in range(start + 1, end):
        if re.match(rf"^\s*{re.escape(key)}\s*=", lines[i]):
            new_line = f"{key} = {value}"
            if lines[i].strip() == new_line:
                return False
            lines[i] = new_line
            return True

    lines.insert(end, f"{key} = {value}")
    return True


def _ensure_sentry_section_settings(lines: list[str], start: int, end: int) -> bool:
    updated = False
    for i in range(start + 1, end):
        if re.match(r"^\s*command\s*=", lines[i]) and "mcp-remote" not in lines[i]:
            lines[i] = 'command = "mcp-remote"'
            updated = True

        match = re.match(r"^\s*args\s*=\s*(\[.*\])\s*$", lines[i])
        if not match:
            continue
        values = _parse_toml_array(match.group(1))
        if not values:
            continue

        if values and "http" in values[-1]:
            target_url = values[-1]
        elif len(values) > 1:
            target_url = values[-1]
        else:
            target_url = "https://mcp.sentry.dev/mcp"

        lines[i] = f'args = ["{target_url}"]'
        updated = True

    return (
        _ensure_section_key_int(lines, start, end, "startup_timeout_sec", CODEX_MCP_STARTUP_TIMEOUT_SEC)
        or updated
    )


def rewrite_mcp_server_settings(text: str) -> tuple[str, bool]:
    lines = text.splitlines()
    changed = False
    disabled_mcp_servers = _get_disabled_mcp_servers()
    if disabled_mcp_servers:
        lines, removed = _filter_disabled_mcp_sections(lines, disabled_mcp_servers)
        changed = changed or removed

    for section_name in ("mcp_servers.linear", "mcp_servers.sentry"):
        start, end = _find_section_range(lines, section_name)
        if start is None:
            continue
        if section_name == "mcp_servers.sentry":
            changed = _ensure_sentry_section_settings(lines, start, end) or changed
            continue
        changed = (
            _ensure_section_key_int(lines, start, end, "startup_timeout_sec", CODEX_MCP_STARTUP_TIMEOUT_SEC)
            or changed
        )

    if not changed:
        return text, False
    return "\n".join(lines).rstrip() + "\n", True


def build_container_codex_config(source_text: str) -> tuple[str, bool]:
    text = source_text
    if text.lstrip().startswith(CODEX_CONFIG_HEADER):
        text = "\n".join(text.splitlines()[1:]).lstrip()

    updated = False
    text, changed = upsert_root_key(text, "cli_auth_credentials_store", "file")
    updated = updated or changed
    text, changed = rewrite_mcp_urls(text)
    updated = updated or changed
    text, changed = rewrite_web_search_feature(text)
    updated = updated or changed
    text, changed = rewrite_mcp_server_settings(text)
    updated = updated or changed

    if text.strip():
        final = CODEX_CONFIG_HEADER + "\n" + text.lstrip()
    else:
        final = CODEX_CONFIG_HEADER + "\n"
    return final, updated


def ensure_codex_state_links(codex_dir: Path, host_dir: Path) -> None:
    if not host_dir.exists():
        return

    expected = ("auth.json", "history.jsonl", "sessions.jsonl")

    def link_path(dest: Path, target: Path) -> None:
        if dest.is_symlink():
            if dest.resolve() == target.resolve():
                return
            dest.unlink()
        if dest.exists() and not target.exists():
            try:
                target.parent.mkdir(parents=True, exist_ok=True)
                dest.replace(target)
            except OSError as exc:
                log(f"failed to move {dest.name} into host codex dir: {exc}")
                return
        elif dest.exists() and target.exists():
            log(f"skipping {dest.name}: exists locally and in host codex dir")
            return
        try:
            dest.symlink_to(target)
        except OSError as exc:
            log(f"failed to link codex state {dest.name}: {exc}")

    for name in expected:
        link_path(codex_dir / name, host_dir / name)

    for item in host_dir.iterdir():
        if item.name == "config.toml" or item.name in expected:
            continue
        link_path(codex_dir / item.name, item)


def ensure_codex_config() -> None:
    codex_dir = Path(os.environ.get("CODEX_HOME", str(Path.home() / ".codex")))
    host_dir = Path(os.environ.get("CODEX_HOST_HOME", str(Path.home() / ".codex-host")))
    codex_dir.mkdir(parents=True, exist_ok=True)

    codex_config = codex_dir / "config.toml"
    host_config = host_dir / "config.toml"

    if codex_config.exists():
        existing = codex_config.read_text(encoding="utf-8")
        if existing.lstrip().startswith(CODEX_CONFIG_HEADER):
            source_text = host_config.read_text(encoding="utf-8") if host_config.exists() else ""
            new_text, updated = build_container_codex_config(source_text)
            if new_text != existing:
                codex_config.write_text(new_text, encoding="utf-8")
                log(f"updated container codex config at {codex_config}")
            else:
                log(f"container codex config already up to date at {codex_config}")
            if updated:
                log("applied container codex overrides (auth store + MCP host rewrite)")
            return

        new_text, updated = build_container_codex_config(existing)
        if new_text != existing:
            codex_config.write_text(new_text, encoding="utf-8")
            log(f"patched codex config at {codex_config}")
            if updated:
                log("applied container codex overrides (auth store + MCP host rewrite)")
        else:
            log(f"codex config already configured at {codex_config}")
        return

    source_text = host_config.read_text(encoding="utf-8") if host_config.exists() else ""
    new_text, updated = build_container_codex_config(source_text)
    codex_config.write_text(new_text, encoding="utf-8")
    log(f"wrote container codex config to {codex_config}")
    if updated:
        log("applied container codex overrides (auth store + MCP host rewrite)")


def ensure_claude_config() -> None:
    claude_dir = Path(os.environ.get("CLAUDE_CONFIG_DIR", str(Path.home() / ".claude")))
    claude_dir.mkdir(parents=True, exist_ok=True)
    claude_config = claude_dir / "settings.json"
    if claude_config.exists():
        log(f"skipping claude settings (already exists at {claude_config})")
        return

    data = {"permissions": {"defaultMode": "bypassPermissions"}}
    claude_config.write_text(json.dumps(data, indent=2) + "\n", encoding="utf-8")
    log(f"wrote default claude settings to {claude_config}")


def ensure_fish_config() -> None:
    fish_config_dir = (
        Path(
            os.environ.get(
                "XDG_CONFIG_HOME",
                str(Path.home() / ".config"),
            )
        )
        / "fish"
    )
    fish_config_dir.mkdir(parents=True, exist_ok=True)
    fish_config = fish_config_dir / "config.fish"
    if fish_config.exists():
        existing = fish_config.read_text(encoding="utf-8")
        if existing.lstrip().startswith("# default fish config for the devcontainer"):
            fish_config.write_text(FISH_CONFIG, encoding="utf-8")
            log(f"updated default fish config at {fish_config}")
            return
        log(f"skipping fish config (already exists at {fish_config})")
        return

    fish_config.write_text(FISH_CONFIG, encoding="utf-8")
    log(f"wrote default fish config to {fish_config}")


def ensure_shell_aliases() -> None:
    for rc_file in (Path.home() / ".bashrc", Path.home() / ".zshrc"):
        rc_file.parent.mkdir(parents=True, exist_ok=True)
        if rc_file.exists():
            existing = rc_file.read_text(encoding="utf-8")
            if "agent aliases (devcontainer)" in existing:
                continue
            content = existing.rstrip() + "\n\n" + SHELL_ALIASES
            rc_file.write_text(content, encoding="utf-8")
            log(f"updated aliases in {rc_file}")
            continue
        rc_file.write_text(SHELL_ALIASES, encoding="utf-8")
        log(f"wrote aliases to {rc_file}")


def ensure_fish_history() -> None:
    history_volume = Path("/commandhistory")
    history_volume.mkdir(parents=True, exist_ok=True)
    target = history_volume / ".fish_history"

    fish_history = Path.home() / ".local" / "share" / "fish" / "fish_history"
    fish_history.parent.mkdir(parents=True, exist_ok=True)

    if fish_history.is_symlink():
        if fish_history.resolve() == target:
            return
        fish_history.unlink()
        fish_history.symlink_to(target)
        log(f"updated fish history symlink at {fish_history}")
        return

    if fish_history.exists():
        if not target.exists():
            fish_history.replace(target)
            log(f"moved fish history to {target}")
        else:
            log(f"existing fish history left at {fish_history}")
            return

    fish_history.symlink_to(target)
    log(f"linked fish history to {target}")


def ensure_uv_tools() -> None:
    tools = ("ruff", "pytest", "mypy", "prek")
    missing = [tool for tool in tools if shutil.which(tool) is None]
    if not missing:
        log("uv tools already installed")
        return
    if shutil.which("uv") is None:
        log("uv not found; skipping uv tool install")
        return
    log(f"installing uv tools: {', '.join(missing)}")
    failed: list[str] = []
    for tool in missing:
        cmd = ["uv", "tool", "install", tool]
        result = subprocess.run(
            cmd,
            check=False,
            capture_output=True,
            text=True,
        )
        if result.returncode != 0:
            detail = result.stderr.strip() or result.stdout.strip()
            log(f"uv tool install failed for {tool}: {detail}")
            failed.append(tool)
    if failed:
        log(f"uv tool install incomplete (failed: {', '.join(failed)})")
    else:
        log("uv tools installed")


def ensure_mcp_remote() -> None:
    if shutil.which("mcp-remote") is not None:
        return

    npm_bin = shutil.which("npm")
    if npm_bin is None:
        log("npm not found; skipping mcp-remote install")
        return

    log("installing mcp-remote for sentry MCP startup")
    result = subprocess.run(
        [npm_bin, "install", "-g", "mcp-remote"],
        check=False,
        capture_output=True,
        text=True,
    )
    if result.returncode != 0:
        detail = result.stderr.strip() or result.stdout.strip()
        log(f"mcp-remote install failed; sentry MCP will rely on existing config: {detail}")
        return

    log("mcp-remote installed")


def ensure_latest_codex() -> None:
    npm_bin = shutil.which("npm")
    if npm_bin is None:
        log("npm not found; skipping codex update")
        return

    result = subprocess.run(
        [npm_bin, "install", "-g", "@openai/codex@latest"],
        check=False,
        capture_output=True,
        text=True,
    )
    if result.returncode != 0:
        detail = result.stderr.strip() or result.stdout.strip()
        log(f"codex update check failed: {detail}")
        return

    log("codex is up to date")


def ensure_latest_takopi() -> None:
    if shutil.which("uv") is None:
        log("uv not found; skipping takopi update")
        return

    result = subprocess.run(
        ["uv", "tool", "install", "-U", "takopi"],
        check=False,
        capture_output=True,
        text=True,
    )
    if result.returncode != 0:
        detail = result.stderr.strip() or result.stdout.strip()
        log(f"takopi update check failed: {detail}")
        return

    log("takopi is up to date")


def _node_modules_ready(path: Path) -> bool:
    node_modules = path / "node_modules"
    if not node_modules.is_dir():
        return False
    try:
        return any(node_modules.iterdir())
    except OSError:
        return True


def _read_package_manager(path: Path) -> str | None:
    package_json = path / "package.json"
    if not package_json.exists():
        return None
    try:
        data = json.loads(package_json.read_text(encoding="utf-8"))
    except (OSError, json.JSONDecodeError):
        return None
    package_manager = data.get("packageManager")
    if isinstance(package_manager, str):
        return package_manager
    return None


def _bun_lock_present(path: Path) -> bool:
    return (path / "bun.lockb").exists() or (path / "bun.lock").exists()


def _uses_bun(path: Path) -> bool:
    if _bun_lock_present(path):
        return True
    package_manager = _read_package_manager(path)
    return bool(package_manager and package_manager.startswith("bun@"))


def _bun_install_args(path: Path) -> list[str]:
    if _bun_lock_present(path):
        return ["--frozen-lockfile"]
    return []

def _read_workspaces_from_package_json(workspace: Path) -> list[str]:
    package_json = workspace / "package.json"
    if not package_json.exists():
        return []
    try:
        data = json.loads(package_json.read_text(encoding="utf-8"))
    except (OSError, json.JSONDecodeError):
        return []
    workspaces = data.get("workspaces")
    if isinstance(workspaces, list):
        return [item for item in workspaces if isinstance(item, str)]
    if isinstance(workspaces, dict):
        packages = workspaces.get("packages")
        if isinstance(packages, list):
            return [item for item in packages if isinstance(item, str)]
    return []


def _read_workspaces_from_pnpm(path: Path) -> list[str]:
    if not path.exists():
        return []
    try:
        lines = path.read_text(encoding="utf-8").splitlines()
    except OSError:
        return []

    patterns: list[str] = []
    in_packages = False
    base_indent = 0

    for line in lines:
        stripped = line.strip()
        if not stripped or stripped.startswith("#"):
            continue
        if not in_packages:
            if stripped.startswith("packages:"):
                in_packages = True
                base_indent = len(line) - len(line.lstrip())
                inline = stripped[len("packages:"):].strip()
                if inline:
                    if inline.startswith("[") and inline.endswith("]"):
                        try:
                            value = ast.literal_eval(inline)
                        except Exception:
                            value = None
                        if isinstance(value, list):
                            for item in value:
                                if isinstance(item, str):
                                    patterns.append(item)
                    else:
                        if (inline.startswith("'") and inline.endswith("'")) or (
                            inline.startswith('"') and inline.endswith('"')
                        ):
                            inline = inline[1:-1]
                        patterns.append(inline)
                continue
        else:
            indent = len(line) - len(line.lstrip())
            if indent <= base_indent:
                in_packages = False
                continue
            if stripped.startswith("-"):
                item = stripped[1:].strip()
                if (item.startswith("'") and item.endswith("'")) or (
                    item.startswith('"') and item.endswith('"')
                ):
                    item = item[1:-1]
                if item:
                    patterns.append(item)

    return patterns


def _normalize_workspace_pattern(pattern: str) -> str:
    normalized = pattern.strip()
    if normalized.startswith("./"):
        normalized = normalized[2:]
    return normalized


def _collect_workspace_targets(workspace: Path, patterns: list[str]) -> list[Path]:
    includes: list[str] = []
    excludes: list[str] = []

    for raw in patterns:
        normalized = _normalize_workspace_pattern(raw)
        if not normalized:
            continue
        if normalized.startswith("!"):
            excluded = normalized[1:].strip()
            if excluded:
                excludes.append(excluded)
        else:
            includes.append(normalized)

    if not includes:
        return []

    candidates: set[Path] = set()
    for pattern in includes:
        for match in workspace.glob(pattern):
            if match.is_file() and match.name == "package.json":
                match = match.parent
            if match.is_dir():
                candidates.add(match)

    if excludes:
        excluded_paths: set[Path] = set()
        for pattern in excludes:
            for match in workspace.glob(pattern):
                if match.is_file() and match.name == "package.json":
                    match = match.parent
                if match.is_dir():
                    excluded_paths.add(match)
        candidates -= excluded_paths

    results: list[Path] = []
    for path in candidates:
        if "node_modules" in path.parts:
            continue
        if not (path / "package.json").exists():
            continue
        try:
            relative = path.relative_to(workspace)
        except ValueError:
            continue
        if relative.as_posix() == ".":
            continue
        results.append(path)

    return sorted(results, key=lambda item: item.as_posix())


def _iter_bun_targets(workspace: Path) -> list[Path]:
    targets: list[Path] = []
    if (workspace / "package.json").exists():
        targets.append(workspace)

    patterns: list[str] = []
    patterns.extend(_read_workspaces_from_package_json(workspace))
    patterns.extend(_read_workspaces_from_pnpm(workspace / "pnpm-workspace.yaml"))
    patterns.extend(_read_workspaces_from_pnpm(workspace / "pnpm-workspace.yml"))

    if patterns:
        targets.extend(_collect_workspace_targets(workspace, patterns))

    seen: set[Path] = set()
    deduped: list[Path] = []
    for path in targets:
        resolved = path.resolve()
        if resolved in seen:
            continue
        seen.add(resolved)
        deduped.append(path)
    return deduped


def ensure_bun_deps(workspace: Path) -> None:
    if os.environ.get("DEVCONTAINER_SKIP_BUN_INSTALL", "").lower() in ("1", "true", "yes"):
        log("skipping bun install (DEVCONTAINER_SKIP_BUN_INSTALL set)")
        return
    if shutil.which("bun") is None:
        log("bun not found; skipping bun installs")
        return

    for path in _iter_bun_targets(workspace):
        label = str(path.relative_to(workspace)) if path != workspace else "workspace root"
        if not _uses_bun(path):
            log(f"skipping bun install in {label} (no bun lock or packageManager)")
            continue
        if not _bun_lock_present(path):
            log(f"skipping bun install in {label} (missing bun lockfile)")
            continue
        ensure_dir_ownership(path / "node_modules")
        if _node_modules_ready(path):
            log(f"skipping bun install in {label} (node_modules exists)")
            continue
        cmd = ["bun", "install", *_bun_install_args(path)]
        log(f"installing bun deps in {label}")
        result = subprocess.run(
            cmd,
            cwd=path,
            check=False,
            capture_output=True,
            text=True,
        )
        if result.returncode != 0:
            detail = result.stderr.strip() or result.stdout.strip()
            log(f"bun install failed in {label}: {detail}")


def ensure_dir_ownership(path: Path) -> None:
    path.mkdir(parents=True, exist_ok=True)
    try:
        stat = path.stat()
    except OSError as exc:
        log(f"unable to stat {path}: {exc}")
        return

    uid = os.getuid()
    gid = os.getgid()
    if stat.st_uid == uid and stat.st_gid == gid:
        return

    result = run_sudo(["chown", "-R", f"{uid}:{gid}", str(path)])
    if result.returncode != 0:
        log(f"failed to chown {path}: {result.stderr.strip()}")
        return
    log(f"fixed ownership for {path}")


def install_tmux_config() -> None:
    tmux_dest = Path.home() / ".tmux.conf"
    if tmux_dest.exists():
        log(f"skipping tmux config (already exists at {tmux_dest})")
        return

    tmux_dest.write_text(TMUX_CONFIG, encoding="utf-8")
    log(f"installed tmux config to {tmux_dest}")


def main() -> None:
    workspace = resolve_workspace()
    if not is_git_repo(workspace):
        log(f"skipping git repo checks (no repo at {workspace})")

    install_tmux_config()
    ensure_dir_ownership(Path("/commandhistory"))
    ensure_dir_ownership(Path.home() / ".claude")
    ensure_dir_ownership(Path.home() / ".codex")
    ensure_dir_ownership(Path.home() / ".takopi")
    ensure_dir_ownership(Path.home() / ".config" / "gh")
    uv_env = os.environ.get("UV_PROJECT_ENVIRONMENT")
    if uv_env:
        ensure_dir_ownership(Path(uv_env))
    ensure_fish_history()
    ensure_global_gitignore(workspace)
    disabled_mcp_servers = _get_disabled_mcp_servers()
    if "sentry" not in disabled_mcp_servers:
        ensure_mcp_remote()
    ensure_codex_config()
    ensure_latest_codex()
    ensure_codex_state_links(
        Path(os.environ.get("CODEX_HOME", str(Path.home() / ".codex"))),
        Path(os.environ.get("CODEX_HOST_HOME", str(Path.home() / ".codex-host"))),
    )
    ensure_claude_config()
    ensure_fish_config()
    ensure_shell_aliases()
    ensure_uv_tools()
    ensure_latest_takopi()
    ensure_bun_deps(workspace)
    log("configured defaults for container use")


if __name__ == "__main__":
    main()
